---
title: "set2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(fpp)
```

### 2. Consider the daily closing IBM stock prices (data set ibmclose).

> (a) Produce some plots of the data in order to become familiar with it

```{r}
plot(ibmclose)
```

> (b) Split the data into a training set of 300 observations and a test set of 69 observations.

```{r}
training <- window(ibmclose, end=300)
test <- window(ibmclose, start=301)
```

> (c) Try various benchmark methods to forecast the training set and compare the results on the
test set. Which method did best?

```{r}
ibmclose_fit1 <- meanf(training, h=69)
ibmclose_fit2 <- rwf(training, h=69, drift=TRUE)
ibmclose_fit3 <- naive(training, h=69)
plot(test, ylim=c(330,510))
lines(ibmclose_fit1$mean, col=2)
lines(ibmclose_fit2$mean, col=3)
lines(ibmclose_fit3$mean, col=4)
legend("topright", lty=1, col=c(2,3,4),
       legend=c("meanf", "drift", "naive"))

accuracy(ibmclose_fit1, test)
accuracy(ibmclose_fit2, test)
accuracy(ibmclose_fit3, test)
```

O segundo e o terceiro método obtiveram uma acúracia muito parecida, porém o terceiro método (naive) obteve um desempenho um pouco melhor.

> d) For the best method, compute the residuals and plot them. What do the plots tell you?

```{r}
res <- residuals(ibmclose_fit3)
plot(res)
hist(res, breaks="FD")
shapiro.test(res)
```

Temos ausência de normalidade pelo teste de shapiro (p-value < 0.05).

```{r}
Acf(res)
Box.test(res, type="Lj")
```

E os dados não são white-noise (p-value < 0.05)

### 3. Consider the sales of new one-family houses in the USA (Jan 1987 – Nov 1995). Data set: hsales

> (a) Produce some plots of the data in order to become familiar with it

```{r}
hsales1 <- window(hsales, start=c(1987,1))
plot(hsales1)
```

> (b) Split the data into a training set and a test set, where the test set is the last two years of data

```{r}
test <- window(hsales1, start=c(1994))
training <- window(hsales1, end=c(1993, 12))
```

> (c) Try various benchmark methods to forecast the training set and compare the results on the
test set. Which method did best?

```{r}
training_fit1 <- meanf(training, h=7*12)
training_fit2 <- rwf(training, h=7*12, drift=TRUE)
training_fit3 <- naive(training, h=7*12)
training_fit4 <- snaive(training, h=7*12)
plot(test)
lines(training_fit1$mean, col=2)
lines(training_fit2$mean, col=3)
lines(training_fit3$mean, col=4)
lines(training_fit4$mean, col=5)
legend("topright", lty=1, col=c(2,3,4,5),
       legend=c("meanf", "drift", "naive", "season naive"))

accuracy(training_fit1, test)
accuracy(training_fit2, test)
accuracy(training_fit3, test)
accuracy(training_fit4, test)
```

O método seasonal naive é o melhor.

> (d) For the best method, compute the residuals and plot them. What do the plots tell you?

```{r}
res <- residuals(training_fit4)
plot(res)
hist(res, breaks="FD")
shapiro.test(res)
``` 

Possui ausencia de normalidade (p-value > 0.05) então a possibilidade de estar normalmente distribuido não pode ser descartada 
[[??]]

```{r}
Box.test(res, type="Lj")
```

Os dados não pertencem a white-noise
[[porém o método pareceu bom?]]